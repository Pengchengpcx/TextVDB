<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Textualize Visual Prompt for Image Editing via Diffusion Bridge</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script>MathJax = {tex: {inlineMath: [['$', '$'],['$$', '$$'], ['\\(', '\\)']]}}</script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Textualize Visual Prompt for Image Editing via Diffusion Bridge</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/Pengchengpcx" target="_blank">Pengcheng Xu</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://fqnchina.github.io/" target="_blank">Qingnan Fan</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.sg/citations?user=NL26uJYAAAAJ&hl=en" target="_blank">Fei Kou</a><sup>2</sup>,</span>
                    <a href="" target="_blank">Shuai Qin</a><sup>2</sup>,</span>
                    <a href="" target="_blank">Hong Gu</a><sup>2</sup>,</span>
                    <a href="" target="_blank">Ruoyu Zhao</a><sup>2,3</sup>,</span>
                    <a href="https://www.csd.uwo.ca/~xling/" target="_blank">Charles Ling</a><sup>1</sup>,</span>
                    <a href="https://sites.google.com/site/borriewang/" target="_blank">Boyu Wang</a><sup>1</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Western University &#20;</span>
                    <span class="author-block"><sup>2</sup>VIVO &#20;</span>
                    <span class="author-block"><sup>3</sup>Xidian University</span>
                  </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github Link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/Pengchengpcx/TextVDB" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Github</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.03495" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <div class="container">
          <div class="item item-steve">
            <img src="./static/images/teaser.png"  width="100%">
          </div>
        </div>
      <h2 class="subtitle has-text-justified">
        The visual prompt defines the visual transformation, which is difficult to describe accurately by language, by a before-and-after image pair. Our method learns such delicate transformation into pseudo text (&lt;A&gt; and &lt;C&gt;), supports hybrid editing with natural text, and can control the intensity of editing with rigorous consistency.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser-->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual prompt, a pair of before-and-after edited images, can convey indescribable imagery transformations and prosper in image editing. However, current visual prompt methods rely on a pretrained text-guided image-to-image generative model that requires a triplet of text, before, and after images for retraining over a text-to-image model. Such crafting triplets and retraining processes limit the scalability and generalization of editing. In this paper, we present a framework based on any single text-to-image model without reliance on the explicit image-to-image model thus enhancing the generalizability and scalability. Specifically, by leveraging the probability-flow ordinary equation, we construct a diffusion bridge to transfer the distribution between before-and-after images under the text guidance. By optimizing the text via the bridge, the framework adaptively textualizes the editing transformation conveyed by visual prompts into text embeddings without other models. Meanwhile, we introduce differential attention control during text optimization, which disentangles the text embedding from the invariance of the before-and-after images and makes it solely capture the delicate transformation and generalize to edit various images. Experiments on real images validate competitive results on the generalization, contextual coherence, and high fidelity for delicate editing with just one image pair as the visual prompt.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->







  <!-- Framework-->
  <section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <h2 class="title is-3">Learning Delicate Visual Transformation via Diffusion Bridge</h2>
          </div>
            <div class="columns is-centered has-text-left">
                <div class="column">
                    <div class="card">
                        <div class="card-image js-modal-trigger" data-target="modal-sample7">
                            <img loading="lazy" src="static/images/framework.png" alt="sample7" />
                        </div>
                        <div class="card-content"><strong>Left</strong>: The before-image is first transferred to a deterministic latent encoding via the unconditional model and then to the after-image under the text guidance. The text embeddings are optimized with fixed start (latent $\mathbf{x}_T$) and end (after-image $\mathbf{x}_0^a$) states. <strong>Right</strong>: In training, the attention of the before-image $M_t^b$ is first timed with the column-transformation matrix $\mathbf{\Lambda}$ to switch the column of <end> token, then masked with $\mathbf{F}$. The attention of the after-image $M_t^a$ is masked with $1-\mathbf{F}$ to get the attention of the $y$ tokens. The final $M_t$ is the addition of two masked attentions. This preserves the linguistic format of the cross-attention and enables the embedding to learn disentangled and generalized visual transformation.
                            </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>




<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content has-text-centered">
          <h2 class="title is-3">Qualitative Comparisons on Real Images</h2>
          <center>
          <img src="static/images/qualitative.png" alt="Inference Overview" class="center-image"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Visual prompts with different editing types and different levels of geometric changes. Our method generalizes to different editing types and scenes while preserving different levels of geometric structures.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!--    BibTex citation-->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
      <h2 class="title"><a id="bibtex">BibTeX</a></h2>
      <pre><code>@article{xu2025textualize,
      title={Textualize Visual Prompt for Image Editing via Diffusion Bridge},
      author={Xu, Pengcheng and Fan, Qingnan and Kou, Fei and Qin, Shuai and Gu, Hong and Zhao, Ruoyu and Ling, Charles and Wang, Boyu},
      booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
      year={2025}
      }</code></pre>
  </div>
</section>
<!--    End BibTex citation-->



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
